{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gambler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDPの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from general import MDP\n",
    "\n",
    "class MDP_Gambler(MDP):\n",
    "\n",
    "    def __init__(self, ph, goal):\n",
    "        self.ph = ph\n",
    "        self.goal = goal\n",
    "        states = list(range(self.goal + 1))\n",
    "        actions = list(range(1, self.goal))\n",
    "        super().__init__(states, actions, 1.0)\n",
    "\n",
    "    def dyn(self, s, a):\n",
    "        if s == 0: return []\n",
    "        if s == self.goal: return []\n",
    "        if a > s: return []\n",
    "        if s + a > self.goal: return []\n",
    "        return [(self.ph, s + a, 1 if s + a == self.goal else 0),\n",
    "                (1 - self.ph, s - a, 0)]\n",
    "    \n",
    "    def opt_pol_cov(self, vfn, thr=0.01):\n",
    "        def act_val(s):\n",
    "            acts = list(range(1, min(s, self.goal - s) + 1))\n",
    "            vs = [self.ph * (1 if s + a == self.goal else vfn[s + a]) + (1 - self.ph) * vfn[s - a] \n",
    "                  for a in acts]\n",
    "            vMax = max(vs)\n",
    "            aList = [a for a in acts if vs[a-1] > vMax - thr]\n",
    "            prob = 1 / len(aList)\n",
    "            return [(prob, a) for a in aList]\n",
    "        return {s : act_val(s) for s in range(1, self.goal)}\n",
    "    \n",
    "    def show_pol_cov(self, pol):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for s in range(1, self.goal):\n",
    "            for (_, a) in pol[s]:\n",
    "                xs.append(s)\n",
    "                ys.append(a)\n",
    "        plt.scatter(np.array(xs), np.array(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(ph, goal, thr):\n",
    "    # インスタンス作成\n",
    "    gambler = MDP_Gambler(ph, goal)\n",
    "    # 最適価値関数の計算\n",
    "    (_, vfn, _) = gambler.value_iter(thr=thr)\n",
    "    # ライブラリとして用意した value_iter() が返すポリシーは，最適アクションのうち任意の\n",
    "    # 1つを選択する．ここでは，opt_pol_cov() なるメソッドを用意した．これは，最適\n",
    "    # アクションのすべてを等確率で選択する．\n",
    "    pol = gambler.opt_pol_cov(vfn, thr=10*thr)\n",
    "    # 選択されたアクションを表示する．\n",
    "    gambler.show_pol_cov(pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.4, 100, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんだ，これは? もう少し横軸の量を減らして goal = 16 とすると，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.4, 16, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふーん?  goal = 4 にしてみると?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.4, 4, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = 1, 3 は，他には選択肢はない．x = 2 のときは，1賭けるより2賭けた方が見込みがある．2賭ければ 0.4 で成功確定だが，\n",
    "1賭けると，その0.4で勝った後，もう一回 0.4 の幸運を得ないといけない．もちろん，賭け額を1に押さえておけば，負けたときにももう一度チャンスがある．\n",
    "しかし，各回の確率は0.4で半分以下なのだから，長くゲームを続ければ総合的にじり貧なので，一気に賭けた方が良い，と思えば良いだろうか?\n",
    "\n",
    "この戦略では，成功確率は， \n",
    "$p_2 = p_h(=0.4)$, $p_1 = p_h p_2 = {p_h}^2$,  $p_3 = p_h + (1-p_h)p_2 = 2p_h + {p_h}^2$ である．\n",
    "\n",
    "もう一つ増やして goal = 8 とすると，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.4, 8, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは不思議な結果である．n = 2, 4，6 は良いとして，なぜ n = 3, 5 は，候補が2つあるのだろうか?\n",
    "\n",
    "goal = 4 の結果から，$p_2 = {p_h}^2$, $p_4 = p_h$, $p_6 = 2p_h - {p_h}^2$ であるから，n = 3 のときに，3賭けるとすると，成功確率は，\n",
    "$p_hp_6 = 2{p_h}^2 - {p_h}^3$，1賭けるとすると，\n",
    "$p_hp_4 + (1-p_h)p_2 = {p_h}^2 + (1-p_h){p_h}^2 = 2{p_h}^2 - {p_h}^3$ で，完全に一致する，というわけである．\n",
    "\n",
    "計算は確かにこうなのだけれど，直観的な説明はつかないのだろうか? 「じり貧」理論は成り立たないということか．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_h > 0.5$ の時には，じり貧理論の逆側を考えると，できるだけ安全に小さく賭ける，つまり，毎回1ずつ賭けるのが良いように思われる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.6, 100, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またまた予想が外れたように見えるが，これは，そういうわけではないようである．次の結果を参照．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.6, 100, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "山が小さくなった．もっと threshold を小さくしてみたいところであるが，あまり小さくするとゼロ割エラーになってしまう．\n",
    "いずれ誤差の話であるようだ．nを小さくして実験してみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.6, 32, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0.6, 32, 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "というわけで，こちらは直観と一致した結果となっているようだ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
